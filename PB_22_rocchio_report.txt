We have used the file PAT2_22_ranked_list_A.csv that was generated in Part A Task 2A as it is the file that contains the ranked list in lnc.ltc scheme that was generated by giving the queries to our IR system.

Without any feedback, the we get the following metrices from the lnc.ltc scheme:

mAP@20 -> 0.5443102868951317
NDGC@20 -> 0.6933311285986438

-------------------------------------------------------------------------------

For the top 20 documents in the retrieved ranked list, and considering only those documents with relevance score of 2 as relevant and rest non-relevant, the modified query vector is generated using Relevance Feedback(RF)

For the different values of α, β and γ, we get the following metrices:

1. α = 1, β = 1 and γ = 0. 5
mAP@20 -> 0.7197260537545623
NDGC@20 -> 0.9070121392734898

2. α = 0. 5, β = 0. 5 and γ = 0. 5
mAP@20 -> 0.7592545732614908
NDGC@20 -> 0.9187818213668147

3. α = 1, β = 0. 5 and γ = 0
mAP@20 -> 0.705369661914789
NDGC@20 -> 0.8946839068877904

-------------------------------------------------------------------------------

Now, considering only the top 10 as relevant and the rest as null, Pseudo Relevance Feedback(PsRF) is implemented and the query vector is modified accordingly.

For the different values of α, β and γ, we get the following metrices:

1. α = 1, β = 1 and γ = 0. 5
mAP@20 -> 0.581345366454525
NDGC@20 -> 0.7113234627453381

2. α = 0. 5, β = 0. 5 and γ = 0. 5
mAP@20 -> 0.581345366454525
NDGC@20 -> 0.7113234627453381

3. α = 1, β = 0. 5 and γ = 0
mAP@20 -> 0.5743746264494768
NDGC@20 -> 0.7049559375618498

-------------------------------------------------------------------------------

We can clearly see in case of Relevance Feedback and Pseudo Relevance Feedback, both have better mAP@20 and NDGC@20 for all settings of α, β and γ as compared to the lnc.ltc scheme without any feedback. This is because the recall and precision is increased due to feedback which gives us more relevant documents at the top.

Now we can see that the mAP@20 and NDGC@20 are better in case of Relevance Feedback than in Pseudo Relevance Feedback. Since in Pseudo Relevance Feedback, the manual part of Relevance Feedback is automated, it works well on average. But for some queries, it can go horribly wrong since there is no manual checking whether the feedback is correct or not. This reduces the metrices value in those queries, thus reducing the overall average.
Relevance Feedback does better because an actual user provides the feedback to the IR system, thus is accurate. Although it consumes time of the user and users might not always provide feedback.

Also, we see that a tradeoff has to be made between α and β/γ values. We would ideaaly like to maximize β/γ, but we can't let the α value to be higher. Thus, we can see that there is a better recall in case of α = 0. 5, β = 0. 5 and γ = 0. 5 in both RF and PsRF metrices. Precision may or may not improve when there is an improvement in recall(which can be judged by NDGC values as it tries to balance out the recall and precision values).